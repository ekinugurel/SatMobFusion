{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "# Import necessary geometric objects from shapely module\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "import skmob\n",
    "import fiona\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Partionining\n",
    "* We have a few options in partionining mobile data spatially\n",
    "    * __Option 1__: Call a free geocoding API (i.e. Nominatim) to get the relevant OpenStreetMap information. From here we can find the bounding box coordinates of a particular POI\n",
    "    * __Option 2__: Partition by federal and/or state designations, i.e. census tracts, zip codes, etc.\n",
    "    * __Option 3__: Type in a bounding box manually. For example, [geojson.io](https://geojson.io) could be a good option.\n",
    "    * __Option 4__: Obtain a KMZ file from the [Damage Assessment Toolkit](https://apps.dat.noaa.gov/stormdamage/damageviewer/>)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locator = Nominatim(user_agent=\"Untitled-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = locator.geocode(\"University of Washington, Seattle, WA, USA\")\n",
    "location.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = list()\n",
    "[bbox.append(float(i)) for i in location.raw['boundingbox']]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "miny, maxy, minx, maxx = bbox\n",
    "poly = box(minx=minx, miny=miny, maxx=maxx, maxy=maxy)\n",
    "poly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Census Tract Example\n",
    "* The bounding box of a census tract is also the convex hull of its geometry object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OK_url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_40_tract_500k.zip\"\n",
    "OK_tracts = gpd.read_file(OK_url)\n",
    "OK_tracts.geometry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OK_tracts.geometry[0].convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IL_url = \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_17_tract_500k.zip\"\n",
    "#IL_tracts = gpd.read_file(IL_url)\n",
    "#IL_tracts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's read in our mobile data\n",
    "* IL first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IL_data = pd.read_csv('IL_1000_April06-08.csv')\n",
    "#IL_gpd = gpd.GeoDataFrame(IL_data, crs='EPSG:4269', geometry=gpd.points_from_xy(IL_data['lon'], IL_data['lat']))\n",
    "#aea_proj_str = '+proj=aea +lat_1=38.00 +lat_2=42.00 +lat_0=40.11 +lon_0=-88.228'\n",
    "#IL_gpd = IL_gpd.to_crs(aea_proj_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude points outside of Illinois\n",
    "# IL_gpd_j = gpd.sjoin(IL_gpd, IL_tracts, how='inner', predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(IL_gpd.shape[0])\n",
    "#print(IL_gpd_j.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at Tulsa\n",
    "* Mobile data between May 2nd and 22nd, 2020\n",
    "* Tornado event on May 15th, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data as geodataframe, project to CRS\n",
    "OK_data = pd.read_csv('data/mobile/OK_damage_May02-22.csv')\n",
    "OK_gpd = gpd.GeoDataFrame(OK_data, crs='EPSG:4269', geometry=gpd.points_from_xy(OK_data['lon'], OK_data['lat']))\n",
    "\n",
    "# Exclude points outside of Oklahoma\n",
    "OK_gpd_j = gpd.sjoin(OK_gpd, OK_tracts, how='left', predicate='within')\n",
    "\n",
    "# Create datetime column using timestamps\n",
    "OK_gpd_j['datetime'] = pd.to_datetime(OK_gpd_j['timestamp'], unit='s')\n",
    "\n",
    "# Keep first 13 columns\n",
    "OK_gpd_j = OK_gpd_j.iloc[:, :13]\n",
    "# Create a date column\n",
    "OK_gpd_j['date'] = OK_gpd_j['datetime'].dt.date\n",
    "OK_gpd_j.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess mobility data; separate by user_ID, filter out points with speed > 400 km/h, \n",
    "# and compress points within 200 m of each other in the same trajectory\n",
    "tdf = skmob.TrajDataFrame(OK_gpd_j, latitude='lat', longitude='lon', datetime='datetime', user_id='uid')\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=400, include_loops=False)\n",
    "fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=0.2)\n",
    "\n",
    "# Let's see how many points we eliminated\n",
    "print('Original number of points: ', tdf.shape[0])\n",
    "print('Number of points after filtering: ', f_tdf.shape[0])\n",
    "print('Number of points after compression: ', fc_tdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pts = gpd.GeoDataFrame(fc_tdf, crs='EPSG:4269', geometry=gpd.points_from_xy(fc_tdf['lng'], fc_tdf['lat']))\n",
    "damage_area_pts['datetime'] = pd.to_datetime(damage_area_pts['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "damage_area_pts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how many unique users passed by the bounding box area in our timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(damage_area_pts['uid'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can play with the data using .explore(), but need to remove the 'datetime' and 'date' columns (make sure to add them back after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop datetime and date column in order to plot it\n",
    "damage_area_pts.drop(['datetime', 'date'], axis=1, inplace=True)\n",
    "damage_area_pts.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf_tulsa['datetime'] = gdf_tulsa['datetime'].dt.strftime('%Y%m%d%H%M%S')\n",
    "# Create datetime column using timestamps\n",
    "# OK_gpd_j['datetime'] = pd.to_datetime(OK_gpd_j['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pts['datetime'] = pd.to_datetime(damage_area_pts['datetime'], format='%Y:%m:%d %H:%M:%S')\n",
    "# Groupby 'uid' and sort by 'datetime'\n",
    "damage_area_pts = damage_area_pts.groupby('uid').apply(lambda x: x.sort_values('datetime'))\n",
    "\n",
    "# Make date column a string\n",
    "damage_area_pts['date'] = damage_area_pts['datetime'].dt.date.astype(str)\n",
    "\n",
    "# Make hour column a string\n",
    "damage_area_pts['hour'] = damage_area_pts['datetime'].dt.hour.astype(str)\n",
    "\n",
    "# Separate by date\n",
    "damage_area_pre = damage_area_pts[damage_area_pts['date'] <= '2020-05-14']\n",
    "damage_area_dur = damage_area_pts[damage_area_pts['date'] == '2020-05-15']\n",
    "damage_area_post = damage_area_pts[damage_area_pts['date'] >= '2020-05-16']\n",
    "\n",
    "damage_area_pre_tdf = skmob.TrajDataFrame(damage_area_pre, latitude='lat', longitude='lng', datetime='datetime', user_id='uid')\n",
    "damage_area_dur_tdf = skmob.TrajDataFrame(damage_area_dur, latitude='lat', longitude='lng', datetime='datetime', user_id='uid')\n",
    "damage_area_post_tdf = skmob.TrajDataFrame(damage_area_post, latitude='lat', longitude='lng', datetime='datetime', user_id='uid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First metric of interest: Radius of Gyration\n",
    "* This is essentially a measure of the extent of a user's travel\n",
    "* Formally, it is defined as the radius of the great circle encompassing a user's data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.measures.individual import jump_lengths, radius_of_gyration, home_location\n",
    "\n",
    "# Groupby date\n",
    "damage_area_pre_tdf_rg = damage_area_pre_tdf.groupby(['date','hour']).apply(lambda x: radius_of_gyration(x))\n",
    "damage_area_dur_tdf_rg = damage_area_dur_tdf.groupby(['date','hour']).apply(lambda x: radius_of_gyration(x))\n",
    "damage_area_post_tdf_rg = damage_area_post_tdf.groupby(['date','hour']).apply(lambda x: radius_of_gyration(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pre_tdf_rg_df = damage_area_pre_tdf_rg.groupby(['date','hour'], level=(0, 1)).mean()\n",
    "damage_area_pre_tdf_rg_df['std'] = damage_area_pre_tdf_rg.groupby(['date','hour'], level=(0, 1)).std()\n",
    "damage_area_pre_tdf_rg_df['uid_count'] = damage_area_pre_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['uid']\n",
    "\n",
    "damage_area_dur_tdf_rg_df = damage_area_dur_tdf_rg.groupby(['date','hour'], level=(0, 1)).mean()\n",
    "damage_area_dur_tdf_rg_df['std'] = damage_area_dur_tdf_rg.groupby(['date','hour'], level=(0, 1)).std()\n",
    "damage_area_dur_tdf_rg_df['uid_count'] = damage_area_dur_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['uid']\n",
    "\n",
    "damage_area_post_tdf_rg_df = damage_area_post_tdf_rg.groupby(['date','hour'], level=(0, 1)).mean()\n",
    "damage_area_post_tdf_rg_df['std'] = damage_area_post_tdf_rg.groupby(['date','hour'], level=(0, 1)).std()\n",
    "damage_area_post_tdf_rg_df['uid_count'] = damage_area_post_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pre_tdf_rg_df = damage_area_pre_tdf_rg_df.reset_index()\n",
    "damage_area_dur_tdf_rg_df = damage_area_dur_tdf_rg_df.reset_index()\n",
    "damage_area_post_tdf_rg_df = damage_area_post_tdf_rg_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime column\n",
    "damage_area_pre_tdf_rg_df['datetime'] = pd.to_datetime(damage_area_pre_tdf_rg_df['date'] + ' ' + damage_area_pre_tdf_rg_df['hour'] + ':00:00')\n",
    "damage_area_dur_tdf_rg_df['datetime'] = pd.to_datetime(damage_area_dur_tdf_rg_df['date'] + ' ' + damage_area_dur_tdf_rg_df['hour'] + ':00:00')\n",
    "damage_area_post_tdf_rg_df['datetime'] = pd.to_datetime(damage_area_post_tdf_rg_df['date'] + ' ' + damage_area_post_tdf_rg_df['hour'] + ':00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just group by hour\n",
    "damage_area_pre_tdf_rg_df_hour = damage_area_pre_tdf_rg_df.sort_values(by=['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all radius of gyrations them in the same axes, using date as x-axis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "damage_area_pre_tdf_rg_df.plot(x='datetime', y='radius_of_gyration',  ax=ax[0], label='Before May 15th')\n",
    "damage_area_dur_tdf_rg_df.plot(x='datetime', y='radius_of_gyration',  ax=ax[0], label='May 15th')\n",
    "damage_area_post_tdf_rg_df.plot(x='datetime', y='radius_of_gyration', ax=ax[0], label='After May 15th')\n",
    "# Fill in the area between the mean and the standard deviation\n",
    "ax[0].fill_between(damage_area_pre_tdf_rg_df['datetime'], damage_area_pre_tdf_rg_df['radius_of_gyration'] - damage_area_pre_tdf_rg_df['std'], damage_area_pre_tdf_rg_df['radius_of_gyration'] + damage_area_pre_tdf_rg_df['std'], alpha=0.3)\n",
    "ax[0].fill_between(damage_area_dur_tdf_rg_df['datetime'], damage_area_dur_tdf_rg_df['radius_of_gyration'] - damage_area_dur_tdf_rg_df['std'], damage_area_dur_tdf_rg_df['radius_of_gyration'] + damage_area_dur_tdf_rg_df['std'], alpha=0.3)\n",
    "ax[0].fill_between(damage_area_post_tdf_rg_df['datetime'], damage_area_post_tdf_rg_df['radius_of_gyration'] - damage_area_post_tdf_rg_df['std'], damage_area_post_tdf_rg_df['radius_of_gyration'] + damage_area_post_tdf_rg_df['std'], alpha=0.3)\n",
    "\n",
    "# Second y-axis to show the number of unique users in each hour\n",
    "#ax2 = ax.twinx()\n",
    "damage_area_pre_tdf_rg_df.plot(x='datetime', y='uid_count', color='black', label='', alpha=0.3, ax=ax[1])\n",
    "damage_area_dur_tdf_rg_df.plot(x='datetime', y='uid_count', color='black', label='', alpha=0.3, ax=ax[1])\n",
    "damage_area_post_tdf_rg_df.plot(x='datetime', y='uid_count',color='black', label='', alpha=0.3, ax=ax[1])\n",
    "#damage_area_pre_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['radius_of_gyration'].plot(ax=ax2, color='black', label='Number of Unique Users')\n",
    "#damage_area_dur_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['radius_of_gyration'].plot(ax=ax2, color='black')\n",
    "#damage_area_post_tdf_rg.groupby(['date','hour'], level=(0, 1)).count()['radius_of_gyration'].plot(ax=ax2, color='black')\n",
    "\n",
    "ax[0].set_title('Radius of Gyration')\n",
    "plt.xlabel('Date')\n",
    "# Share x\n",
    "ax[0].set_ylabel('Radius of Gyration (m)')\n",
    "ax[0].set_ylim((0, None))\n",
    "ax[1].set_ylabel('Number of unique users')\n",
    "ax[1].set_ylim((0, None))\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there doesn't seem to be a clear spike. Besides, this might not be the best metric to analyze in a constrained geographic box. It's likely that many of the users traveling through the tornado area are commuters passing through."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second metric: Visits per time unit (we will group by hour)\n",
    "* This is a clear indicator of whether travel spiked on the timeframe of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.measures.collective import visits_per_time_unit\n",
    "\n",
    "# Groupby date\n",
    "damage_area_pre_tdf_vpt = damage_area_pre_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))\n",
    "damage_area_dur_tdf_vpt = damage_area_dur_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))\n",
    "damage_area_post_tdf_vpt = damage_area_post_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pre_tdf_vpt = damage_area_pre_tdf_vpt.reset_index()\n",
    "damage_area_dur_tdf_vpt = damage_area_dur_tdf_vpt.reset_index()\n",
    "damage_area_post_tdf_vpt = damage_area_post_tdf_vpt.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of visits per time unit on the same axes, as we did with the radius of gyration\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "damage_area_pre_tdf_vpt.plot(x='datetime', y='n_visits', ax=ax, label='Before May 15th')\n",
    "damage_area_dur_tdf_vpt.plot(x='datetime', y='n_visits', ax=ax, label='During May 15th')\n",
    "damage_area_post_tdf_vpt.plot(x='datetime', y='n_visits', ax=ax, label='After May 15th')\n",
    "\n",
    "# Second y-axis to show the number of unique users in each hour\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "ax.set_title('Visits per time unit')\n",
    "plt.xlabel('Date')\n",
    "ax.set_ylabel('Visits per time unit')\n",
    "ax.set_ylim((0, None))\n",
    "#ax[1].set_ylabel('Number of unique users')\n",
    "#ax[1].set_ylim((0, None))\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the trend is not obvious on the daily level, but let's aggregate further by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by hour\n",
    "damage_area_pre_tdf_vpt_df = damage_area_pre_tdf_vpt.groupby(damage_area_pre_tdf_vpt['datetime'].dt.hour).mean()\n",
    "damage_area_pre_tdf_vpt_df['std'] = damage_area_pre_tdf_vpt.groupby(damage_area_pre_tdf_vpt['datetime'].dt.hour).std()['n_visits']\n",
    "\n",
    "damage_area_dur_tdf_vpt_df = damage_area_dur_tdf_vpt.groupby(damage_area_dur_tdf_vpt['datetime'].dt.hour).mean()\n",
    "damage_area_dur_tdf_vpt_df['std'] = damage_area_dur_tdf_vpt.groupby(damage_area_dur_tdf_vpt['datetime'].dt.hour).std()['n_visits']\n",
    "\n",
    "damage_area_post_tdf_vpt_df = damage_area_post_tdf_vpt.groupby(damage_area_post_tdf_vpt['datetime'].dt.hour).mean()\n",
    "damage_area_post_tdf_vpt_df['std'] = damage_area_post_tdf_vpt.groupby(damage_area_post_tdf_vpt['datetime'].dt.hour).std()['n_visits']\n",
    "\n",
    "# Plot the number of visits per time unit on the same axes, as we did with the radius of gyration\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "damage_area_pre_tdf_vpt_df.plot(y='n_visits', ax=ax, label='Before May 15th')\n",
    "damage_area_dur_tdf_vpt_df.plot(y='n_visits',  ax=ax, label='During May 15th')\n",
    "damage_area_post_tdf_vpt_df.plot(y='n_visits',  ax=ax, label='After May 15th')\n",
    "# Fill the area between the mean and the standard deviation\n",
    "ax.fill_between(damage_area_pre_tdf_vpt_df.index, damage_area_pre_tdf_vpt_df['n_visits'] - damage_area_pre_tdf_vpt_df['std'], damage_area_pre_tdf_vpt_df['n_visits'] + damage_area_pre_tdf_vpt_df['std'], alpha=0.2)\n",
    "ax.fill_between(damage_area_dur_tdf_vpt_df.index, damage_area_dur_tdf_vpt_df['n_visits'] - damage_area_dur_tdf_vpt_df['std'], damage_area_dur_tdf_vpt_df['n_visits'] + damage_area_dur_tdf_vpt_df['std'], alpha=0.2)\n",
    "ax.fill_between(damage_area_post_tdf_vpt_df.index, damage_area_post_tdf_vpt_df['n_visits'] - damage_area_post_tdf_vpt_df['std'], damage_area_post_tdf_vpt_df['n_visits'] + damage_area_post_tdf_vpt_df['std'], alpha=0.2)\n",
    "\n",
    "# Add vertical line at 13:37\n",
    "plt.axvline(x=13.37, color='black', linestyle='--', label='Tornado begins')\n",
    "plt.axvline(x=13.49, color='black', linestyle='-', label='Tornado ends')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_title('Visits per time unit')\n",
    "plt.xlabel('Hour')\n",
    "ax.set_ylim((0, None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! There appears to be a clear spike in visits per hour right after the tornado. This is definitely an indicator that something may be up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.measures.individual import location_frequency\n",
    "\n",
    "damage_area_pre_tdf_lf = damage_area_pre_tdf.groupby(['date']).apply(lambda x: location_frequency(x, show_progress=True))\n",
    "damage_area_dur_tdf_lf = damage_area_dur_tdf.groupby(['date']).apply(lambda x: location_frequency(x, show_progress=True))\n",
    "damage_area_post_tdf_lf = damage_area_post_tdf.groupby(['date']).apply(lambda x: location_frequency(x, show_progress=True))\n",
    "\n",
    "damage_area_pre_tdf_lf = damage_area_pre_tdf_lf.reset_index()\n",
    "damage_area_dur_tdf_lf = damage_area_dur_tdf_lf.reset_index()\n",
    "damage_area_post_tdf_lf = damage_area_post_tdf_lf.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting \"stay locations\" in the bounding box. \n",
    "* To qualify, a user needs to spend a certain amount of time within certain radius (the two parameters of the stay detection algorithm)\n",
    "* We can further cluster the detected locations by spatial proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.preprocessing import detection\n",
    "from skmob.preprocessing import clustering\n",
    "\n",
    "pre_stdf = detection.stay_locations(skmob.TrajDataFrame(damage_area_pre_tdf, longitude='lng', latitude='lat', datetime='datetime', user_id='uid'))\n",
    "pre_stdf_c = clustering.cluster(pre_stdf)\n",
    "\n",
    "dur_stdf = detection.stay_locations(skmob.TrajDataFrame(damage_area_dur_tdf, longitude='lng', latitude='lat', datetime='datetime', user_id='uid'))\n",
    "dur_stdf_c = clustering.cluster(dur_stdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_stdf_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahh1 = pd.concat([damage_area_pre_tdf_lf, damage_area_dur_tdf_lf], axis=1)\n",
    "#ahh1 = ahh1[(ahh1.columns != 'uid') & (ahh1.columns != 'geometry')]\n",
    "ahh1.columns = ['date1', 'uid1', 'lat1', 'lng1', 'lf1', 'geometry1', \n",
    "                'date2', 'uid2', 'lat2', 'lng2', 'lf2', 'geometry2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tolerance = 1e-5\n",
    "\n",
    "ahh1[ahh1.apply(lambda x: np.any(np.isclose([x.lat1, x.lng1], \n",
    "                                      [x.lat2, x.lng2], atol=tolerance)), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_dur_tdf_lf.groupby(['lat', 'lng'])['location_frequency'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pre_tdf_lf.groupby(['date'].apply(lambda x: np.all(np.isclose([x.lat, x.lng, ]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_match = metric_tot.apply(lambda x: np.all(np.isclose([x.test_lat, x.test_lng, x[f'test_{metric_name.lower()}']], \n",
    "            [x.pred_lat, x.pred_lng, x[f'pred_{metric_name.lower()}']], atol=tolerance)), axis=1)\n",
    "metric_perc = np.count_nonzero(metric_match) / len(metric_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_area_pre_tdf_lf.groupby(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into geodataframe\n",
    "damage_area_pre_tdf_lf_gdf = gpd.GeoDataFrame(damage_area_pre_tdf_lf, geometry=gpd.points_from_xy(damage_area_pre_tdf_lf['lng'], damage_area_pre_tdf_lf['lat']))\n",
    "damage_area_dur_tdf_lf_gdf = gpd.GeoDataFrame(damage_area_dur_tdf_lf, geometry=gpd.points_from_xy(damage_area_dur_tdf_lf['lng'], damage_area_dur_tdf_lf['lat']))\n",
    "damage_area_post_tdf_lf_gdf = gpd.GeoDataFrame(damage_area_post_tdf_lf, geometry=gpd.points_from_xy(damage_area_post_tdf_lf['lng'], damage_area_post_tdf_lf['lat']))\n",
    "\n",
    "# Plot the first day with random_location_entropy as heatmap\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "damage_area_pre_tdf_lf_gdf.plot(column='location_frequency', ax=axs[0], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "damage_area_dur_tdf_lf_gdf.plot(column='location_frequency', ax=axs[1], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "damage_area_post_tdf_lf_gdf.plot(column='location_frequency', ax=axs[2], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "#damage_area_pre_tdf_rle_gdf\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the same analysis with random location entropy\n",
    "from skmob.measures.collective import random_location_entropy\n",
    "\n",
    "damage_area_pre_tdf_rle = damage_area_pre_tdf.groupby(['date']).apply(lambda x: random_location_entropy(x, show_progress=True))\n",
    "damage_area_dur_tdf_rle = damage_area_dur_tdf.groupby(['date']).apply(lambda x: random_location_entropy(x, show_progress=True))\n",
    "damage_area_post_tdf_rle = damage_area_post_tdf.groupby(['date']).apply(lambda x: random_location_entropy(x, show_progress=True))\n",
    "\n",
    "damage_area_pre_tdf_rle = damage_area_pre_tdf_rle.reset_index()\n",
    "damage_area_dur_tdf_rle = damage_area_dur_tdf_rle.reset_index()\n",
    "damage_area_post_tdf_rle = damage_area_post_tdf_rle.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into geodataframe\n",
    "damage_area_pre_tdf_rle_gdf = gpd.GeoDataFrame(damage_area_pre_tdf_rle, geometry=gpd.points_from_xy(damage_area_pre_tdf_rle['lng'], damage_area_pre_tdf_rle['lat']))\n",
    "damage_area_dur_tdf_rle_gdf = gpd.GeoDataFrame(damage_area_dur_tdf_rle, geometry=gpd.points_from_xy(damage_area_dur_tdf_rle['lng'], damage_area_dur_tdf_rle['lat']))\n",
    "damage_area_post_tdf_rle_gdf = gpd.GeoDataFrame(damage_area_post_tdf_rle, geometry=gpd.points_from_xy(damage_area_post_tdf_rle['lng'], damage_area_post_tdf_rle['lat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random location entropy\n",
    "damage_area_post_tdf_rle_gdf.plot(column='random_location_entropy', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first day with random_location_entropy as heatmap\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "damage_area_pre_tdf_rle_gdf.plot(column='random_location_entropy', ax=axs[0], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "damage_area_dur_tdf_rle_gdf.plot(column='random_location_entropy', ax=axs[1], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "damage_area_post_tdf_rle_gdf.plot(column='random_location_entropy', ax=axs[2], legend=True, cmap='Reds', markersize=0.7, vmin=0, vmax=1)\n",
    "#damage_area_pre_tdf_rle_gdf\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmob.measures.collective import visits_per_time_unit\n",
    "\n",
    "# Groupby date\n",
    "damage_area_pre_tdf_vpt = damage_area_pre_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))\n",
    "damage_area_dur_tdf_vpt = damage_area_dur_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))\n",
    "damage_area_post_tdf_vpt = damage_area_post_tdf.groupby(['date','hour']).apply(lambda x: visits_per_time_unit(x, time_unit='1h'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the comparison sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison sites\n",
    "comp1 = pd.read_csv('data/mobile/OK_comp1_May02-22.csv')\n",
    "\n",
    "comp1_gpd = gpd.GeoDataFrame(comp1, crs='EPSG:4269', geometry=gpd.points_from_xy(comp1['lon'], comp1['lat']))\n",
    "\n",
    "# Exclude points outside of Oklahoma\n",
    "comp1_gpd = gpd.sjoin(comp1_gpd, OK_tracts, how='left', predicate='within')\n",
    "\n",
    "# Create datetime column using timestamps\n",
    "comp1_gpd['datetime'] = pd.to_datetime(comp1_gpd['timestamp'], unit='s')\n",
    "\n",
    "# Keep first 13 columns\n",
    "comp1_gpd = comp1_gpd.iloc[:, :13]\n",
    "# Create a date column\n",
    "comp1_gpd['date'] = comp1_gpd['datetime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess mobility data; separate by user_ID, filter out points with speed > 400 km/h, \n",
    "# and compress points within 200 m of each other in the same trajectory\n",
    "tdf = skmob.TrajDataFrame(comp1_gpd, latitude='lat', longitude='lon', datetime='datetime', user_id='uid')\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=400, include_loops=False)\n",
    "fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=0.2)\n",
    "\n",
    "# Let's see how many points we eliminated\n",
    "print('Original number of points: ', tdf.shape[0])\n",
    "print('Number of points after filtering: ', f_tdf.shape[0])\n",
    "print('Number of points after compression: ', fc_tdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "27d8c82720d960df87743be14161e6f7351af57d8b5f04fea83f01e4b383fdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
